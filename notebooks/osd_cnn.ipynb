{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置工作目录到项目根目录，确保相对路径（`models/` 等）可用。\n",
    "- 固定随机种子，选择计算设备（GPU优先）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: z:\\WorkSpace\\osd-cnn\\notebooks\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Ensure working directory is project root (adjust if needed)\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(\"..\")))\n",
    "os.chdir(project_root)\n",
    "print(f\"CWD: {os.getcwd()}\")\n",
    "\n",
    "# Global seed and device\n",
    "seed = np.random.RandomState(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 生成/加载校验矩阵 `H` 与生成矩阵 `G`，打印 `(n, k)` 与码率。\n",
    "- 设置全局参数：`USE_APP_LLR_FOR_OSD`、`max_iter_nms`、训练SNR、batch大小等。\n",
    "- 指定模型保存路径 `models/dia_cnn.pth`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDPC: n=128, k=67, R=0.52\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pyldpc import make_ldpc, encode, decode, get_message\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# LDPC code params\n",
    "n = 128\n",
    "d_v = 4\n",
    "d_c = 8\n",
    "H, G = make_ldpc(n, d_v, d_c, seed=seed, systematic=True, sparse=True)\n",
    "n_code, k_info = G.shape\n",
    "print(f\"LDPC: n={n_code}, k={k_info}, R={k_info/n_code:.2f}\")\n",
    "\n",
    "# Global configs\n",
    "USE_APP_LLR_FOR_OSD = False\n",
    "max_iter_nms = 12\n",
    "snr_train_db = 2.7\n",
    "epochs = 20\n",
    "batch_size = 256\n",
    "\n",
    "# Model path\n",
    "model_dir = os.path.join(\"models\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, \"dia_cnn.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构建Tanner图邻接，执行规范化最小和（NMS）迭代。\n",
    "- 支持综合校验早停。`alpha` 可按论文调整。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def decode_with_trajectory(H, y, snr, max_iter):\n",
    "    m_checks, n_vars = H.shape\n",
    "\n",
    "    # --- Tanner图邻接表构建 ---\n",
    "    if hasattr(H, 'toarray'): \n",
    "        H_dense = H.toarray()\n",
    "        rows, cols = H.nonzero()\n",
    "    else: \n",
    "        H_dense = H\n",
    "        rows, cols = np.nonzero(H)\n",
    "    \n",
    "    var_to_checks = [[] for _ in range(n_vars)]\n",
    "    check_to_vars = [[] for _ in range(m_checks)]\n",
    "    for r, c in zip(rows, cols):\n",
    "        var_to_checks[c].append(r)\n",
    "        check_to_vars[r].append(c)\n",
    "\n",
    "    # --- 初始LLR计算 ---\n",
    "    R = float(k_info) / float(n_code)\n",
    "    EbN0_linear = 10.0 ** (float(snr) / 10.0)\n",
    "    channel_llr = 4.0 * R * EbN0_linear * y.astype(float)\n",
    "    \n",
    "    llrs_trajectory = np.zeros((n_vars, max_iter + 1), dtype=float)\n",
    "    llrs_trajectory[:, 0] = channel_llr\n",
    "\n",
    "    # --- 消息初始化 ---\n",
    "    # v2c: 变量到校验的消息，初始化为信道LLR\n",
    "    v2c_msgs = np.zeros((m_checks, n_vars), dtype=float)\n",
    "    for v_idx in range(n_vars):\n",
    "        for c_idx in var_to_checks[v_idx]:\n",
    "            v2c_msgs[c_idx, v_idx] = channel_llr[v_idx]\n",
    "    \n",
    "    # c2v: 校验到变量的消息，初始化为0\n",
    "    c2v_msgs = np.zeros((m_checks, n_vars), dtype=float)\n",
    "    alpha = 0.8\n",
    "\n",
    "    # --- 迭代更新 ---\n",
    "    for it in range(1, max_iter + 1):\n",
    "        \n",
    "        # ======================= 关键逻辑修正 =======================\n",
    "        # 阶段一：校验节点更新 (计算 c2v)\n",
    "        # 使用来自上一轮的 v2c_msgs\n",
    "        # ============================================================\n",
    "        for c in range(m_checks):\n",
    "            connected_vars = check_to_vars[c]\n",
    "            if not connected_vars: continue\n",
    "\n",
    "            incoming_v2c = [v2c_msgs[c, v] for v in connected_vars]\n",
    "            \n",
    "            for i, v_target in enumerate(connected_vars):\n",
    "                other_msgs = incoming_v2c[:i] + incoming_v2c[i+1:]\n",
    "                \n",
    "                if not other_msgs:\n",
    "                    c2v_msgs[c, v_target] = 0.0\n",
    "                    continue\n",
    "\n",
    "                signs = np.sign(other_msgs)\n",
    "                abs_values = np.abs(other_msgs)\n",
    "                \n",
    "                sign_prod = np.prod(signs)\n",
    "                min_abs = np.min(abs_values)\n",
    "                \n",
    "                c2v_msgs[c, v_target] = alpha * sign_prod * min_abs\n",
    "\n",
    "        # ============================================================\n",
    "        # 阶段二：变量节点更新 (计算 a_posteriori 和下一轮的 v2c)\n",
    "        # 使用刚刚在本轮计算出的 c2v_msgs\n",
    "        # ============================================================\n",
    "        a_posteriori = channel_llr + np.sum(c2v_msgs, axis=0)\n",
    "        llrs_trajectory[:, it] = a_posteriori\n",
    "\n",
    "        # 计算下一轮要用的 v2c 消息 (外信息)\n",
    "        for v in range(n_vars):\n",
    "            for c in var_to_checks[v]:\n",
    "                 v2c_msgs[c, v] = a_posteriori[v] - c2v_msgs[c, v]\n",
    "\n",
    "        # --- 早停判断 ---\n",
    "        hard_bits = (a_posteriori < 0).astype(int)\n",
    "        if np.all((H.dot(hard_bits) % 2) == 0):\n",
    "            # 将轨迹的剩余部分填充为当前值，以保持形状一致\n",
    "            for remaining_it in range(it + 1, max_iter + 1):\n",
    "                llrs_trajectory[:, remaining_it] = a_posteriori\n",
    "            break\n",
    "            \n",
    "    final_bits = (a_posteriori < 0).astype(int)\n",
    "    return final_bits, llrs_trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 发送全零码字，通过AWGN信道多次采样；\n",
    "- 对每次采样运行 NMS 并记录LLR轨迹；\n",
    "- 仅当NMS失败时，收集每个比特的轨迹作为训练样本，以提升训练有效性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_codeword(H, c):\n",
    "    if c is None:\n",
    "        return False\n",
    "    return np.all((H.dot(c) % 2) == 0)\n",
    "\n",
    "def generate_training_data_from_failures(n_failures_target, snr_db, max_iter_nms):\n",
    "    print(f\"Generating training data from NMS failures at SNR={snr_db} dB...\")\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "\n",
    "    n_failures_found = 0\n",
    "    n_sims_run = 0\n",
    "    \n",
    "    # 创建一个随机数生成器以确保可复现性\n",
    "    rng = np.random.RandomState(1234)\n",
    "\n",
    "    while n_failures_found < n_failures_target:\n",
    "        n_sims_run += 1\n",
    "\n",
    "        # --- 关键修正：每一轮都生成一个随机消息和对应的码字 ---\n",
    "        # 1. 生成随机消息 (k_info, 1)\n",
    "        v_message = rng.randint(2, size=(k_info, 1))\n",
    "        \n",
    "        # 2. 生成纯净的 0/1 码字作为真实标签\n",
    "        G_dense = G.toarray() if hasattr(G, 'toarray') else G\n",
    "        true_codeword_bits = ((v_message.T @ G_dense.T) % 2).flatten().astype(float)\n",
    "        # -----------------------------------------------------------\n",
    "\n",
    "        # 3. 生成带噪声的信道观测值 (注意这里的seed=None，以保证每次噪声不同)\n",
    "        y_noisy = encode(G, v_message, snr_db, seed=None).flatten()\n",
    "        \n",
    "        # 4. 使用修正后的NMS解码器进行解码\n",
    "        decoded_word, llr_trajectory = decode_with_trajectory(H, y_noisy, snr_db, max_iter=max_iter_nms)\n",
    "\n",
    "        if not is_valid_codeword(H, decoded_word):\n",
    "            n_failures_found += 1\n",
    "            for i in range(n_code):\n",
    "                X_train_list.append(llr_trajectory[i, :])\n",
    "                # 标注为 1 表示 predecoded (d_nms / decoded_word) 在该位是错误的（需要纠正），0 否则\n",
    "                err_bit = int(decoded_word[i] != true_codeword_bits[i])\n",
    "                y_train_list.append(err_bit)\n",
    "        if n_sims_run % 5000 == 0:\n",
    "            print(f\"  Sims run: {n_sims_run}, Failures: {n_failures_found}/{n_failures_target}\")\n",
    "\n",
    "    X_train = np.array(X_train_list).reshape(-1, 1, max_iter_nms + 1)\n",
    "    # X_train shape: (N_samples, 1, T)\n",
    "    eps = 1e-6\n",
    "    means = X_train.mean(axis=2, keepdims=True)   # shape (N,1,1)\n",
    "    stds = X_train.std(axis=2, keepdims=True) + eps\n",
    "    X_train = (X_train - means) / stds\n",
    "    y_train = np.array(y_train_list).reshape(-1, 1) \n",
    "    print(f\"Generated {X_train.shape[0]} samples from {n_failures_found} frame failures.\\\\n\")\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 一维卷积堆叠提取轨迹时序特征，输出为单一logit（bit为1的概率的对数几率）。\n",
    "- 输入形状为 `(batch, 1, T)`，其中 `T = max_iter_nms + 1`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready\n"
     ]
    }
   ],
   "source": [
    "class DIA_CNN_Model(nn.Module):\n",
    "    def __init__(self, trajectory_length):\n",
    "        super(DIA_CNN_Model, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # 论文设置: 滤波器数量为 8\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            # 论文设置: 滤波器数量为 4\n",
    "            nn.Conv1d(in_channels=8, out_channels=4, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            # 论文设置: 滤波器数量为 2\n",
    "            nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3, padding='same')\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 输入维度 = 最后一层卷积的通道数 * 轨迹长度\n",
    "        # 2 * trajectory_length -> 1\n",
    "        self.dense = nn.Linear(2 * trajectory_length, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate\n",
    "max_iter_nms = max_iter_nms\n",
    "model = DIA_CNN_Model(trajectory_length=max_iter_nms + 1).to(device)\n",
    "print(\"Model ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果存在 `models/dia_cnn.pth` 则直接加载并跳过训练。\n",
    "- 否则：仅从NMS失败帧采样生成训练数据并训练，训练后保存权重，随后切换到推理模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved model. Generating failures and training...\n",
      "Generating training data from NMS failures at SNR=2.7 dB...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(model_path):\n",
    "    print(f\"Found existing model at {model_path}. Loading...\")\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"Loaded.\\n\")\n",
    "else:\n",
    "    print(\"No saved model. Generating failures and training...\")\n",
    "    X_train_np, y_train_np = generate_training_data_from_failures(\n",
    "        n_failures_target=4000, snr_db=snr_train_db, max_iter_nms=max_iter_nms\n",
    "    )\n",
    "    X_train = torch.from_numpy(X_train_np).float()\n",
    "    y_train = torch.from_numpy(y_train_np).float()\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    n_epochs = 60\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.view(-1, 1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(inputs)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    model.eval()\n",
    "    print(f\"Saved model to {model_path}.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 打印/检查模型对少量示例的输出分布\n",
    "* 在训练完成后，运行下面快速检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_np 取自几条失败样本（或从 X_train 取前 100）\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    x = torch.from_numpy(X_train_np[:256]).float().to(device)  # 已标准化\n",
    "    logits = model(x)\n",
    "    probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "print(\"probs mean,std,min,max:\", probs.mean(), probs.std(), probs.min(), probs.max())\n",
    "# 看一部分预测结果与真实标签对比\n",
    "for i in range(10):\n",
    "    print(f\"pred_prob={probs[i]:.3f}, label={y_train_np[i,0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `osd_rescue_order3`：按可靠度对最不可靠比特进行阶数1/2/3翻转救援（受 `L` 和 `max_triplets` 控制）。\n",
    "- `standard_osd_decoder`：可配置使用信道LLR或NMS最后一轮后验LLR作为可靠度。\n",
    "- `cnn_enhanced_decoder`：利用CNN输出的比特错误概率构造可靠度并执行OSD救援。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def osd_rescue_order3(H, base_decision, reliability_vector, L=12, max_triplets=100):\n",
    "    idx_sorted = np.argsort(reliability_vector)\n",
    "    least_reliable = idx_sorted[:min(L, len(idx_sorted))]\n",
    "    if is_valid_codeword(H, base_decision):\n",
    "        return base_decision\n",
    "    # Order-1\n",
    "    for i in least_reliable:\n",
    "        cand = base_decision.copy(); cand[i] ^= 1\n",
    "        if is_valid_codeword(H, cand):\n",
    "            return cand\n",
    "    # Order-2\n",
    "    for i, j in combinations(least_reliable, 2):\n",
    "        cand = base_decision.copy(); cand[i] ^= 1; cand[j] ^= 1\n",
    "        if is_valid_codeword(H, cand):\n",
    "            return cand\n",
    "    # Order-3 (limited)\n",
    "    triplets_checked = 0\n",
    "    for i, j, k in combinations(least_reliable, 3):\n",
    "        cand = base_decision.copy(); cand[i] ^= 1; cand[j] ^= 1; cand[k] ^= 1\n",
    "        if is_valid_codeword(H, cand):\n",
    "            return cand\n",
    "        triplets_checked += 1\n",
    "        if triplets_checked >= max_triplets:\n",
    "            break\n",
    "    return base_decision\n",
    "\n",
    "\n",
    "def baseline_nms_decoder(H, y, snr):\n",
    "    return decode(H, y, snr, maxiter=max_iter_nms)\n",
    "\n",
    "\n",
    "def standard_osd_decoder(H, y, snr, predecoded=None, llr_traj=None):\n",
    "    decoded = predecoded if predecoded is not None else baseline_nms_decoder(H, y, snr)\n",
    "    if is_valid_codeword(H, decoded):\n",
    "        return decoded\n",
    "    reliability_vec = np.abs(llr_traj[:, -1]) if (USE_APP_LLR_FOR_OSD and llr_traj is not None) else np.abs(y)\n",
    "    return osd_rescue_order3(H, decoded, reliability_vec)\n",
    "\n",
    "\n",
    "def cnn_enhanced_decoder(H, y, snr, model, dev, predecoded=None, llr_traj=None):\n",
    "    if predecoded is None or llr_traj is None:\n",
    "        decoded, llr_trajectory = decode_with_trajectory(H, y, snr, max_iter=max_iter_nms)\n",
    "    else:\n",
    "        decoded, llr_trajectory = predecoded, llr_traj\n",
    "    if is_valid_codeword(H, decoded):\n",
    "        return decoded\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # z-score standardization to match training\n",
    "        mean = llr_trajectory.mean(axis=1, keepdims=True)\n",
    "        std = llr_trajectory.std(axis=1, keepdims=True) + 1e-6\n",
    "        traj_std = (llr_trajectory - mean) / std\n",
    "        traj_tensor = torch.from_numpy(traj_std.T).float().reshape(n_code, 1, -1).to(dev)\n",
    "        logits = model(traj_tensor)\n",
    "        error_probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "    reliability_metric = 1.0 - error_probs\n",
    "    return osd_rescue_order3(H, decoded, reliability_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 批量生成 `n_trials` 帧的接收向量，逐帧运行一次 NMS，收集失败帧。\n",
    "- 标准OSD：仅对失败帧进行救援（可靠度可选信道LLR/后验LLR）。\n",
    "- CNN-OSD：对失败帧的比特轨迹做一次（分块）前向，得到错误概率并执行OSD救援。\n",
    "- 统计三条曲线的BER并绘图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "snrs_db = np.arange(2.0, 4.5, 0.5)\n",
    "n_trials = 20000\n",
    "ber_baseline, ber_std_osd, ber_cnn_osd = [], [], []\n",
    "\n",
    "v_messages = seed.randint(2, size=(k_info, n_trials))\n",
    "\n",
    "for snr in snrs_db:\n",
    "    start_time = time.time()\n",
    "    total_errors_baseline, total_errors_std_osd, total_errors_cnn_osd = 0, 0, 0\n",
    "\n",
    "    y_noisy_batch = encode(G, v_messages, snr, seed=seed)\n",
    "\n",
    "    failed_indices, failed_trajectories, failed_decoded_words = [], [], []\n",
    "\n",
    "    # Stage 1: NMS and Standard OSD\n",
    "    for i in range(n_trials):\n",
    "        y_col, v_col = y_noisy_batch[:, i], v_messages[:, i]\n",
    "        d_nms, llr_traj = decode_with_trajectory(H, y_col, snr, max_iter=max_iter_nms)\n",
    "\n",
    "        errors_this = np.count_nonzero(get_message(G, d_nms) != v_col)\n",
    "        total_errors_baseline += errors_this\n",
    "\n",
    "        if not is_valid_codeword(H, d_nms):\n",
    "            failed_indices.append(i)\n",
    "            failed_trajectories.append(llr_traj)\n",
    "            failed_decoded_words.append(d_nms)\n",
    "\n",
    "            reliability_vec = np.abs(llr_traj[:, -1]) if USE_APP_LLR_FOR_OSD else np.abs(y_col)\n",
    "            d_std_osd = osd_rescue_order3(H, d_nms, reliability_vec)\n",
    "            total_errors_std_osd += np.count_nonzero(get_message(G, d_std_osd) != v_col)\n",
    "        else:\n",
    "            total_errors_std_osd += errors_this\n",
    "\n",
    "    # Start CNN-OSD from baseline errors\n",
    "    total_errors_cnn_osd = total_errors_baseline\n",
    "\n",
    "    # Stage 2: CNN rescue on failed frames\n",
    "    if len(failed_indices) > 0:\n",
    "        print(f\"SNR={snr:.2f} dB, NMS failures: {len(failed_indices)}/{n_trials}\")\n",
    "        batch_trajectories = np.array(failed_trajectories)\n",
    "        flat_trajectories = batch_trajectories.reshape(-1, 1, max_iter_nms + 1)\n",
    "\n",
    "        # Optional: chunking to avoid OOM\n",
    "        with torch.no_grad():\n",
    "            logits_list = []\n",
    "            chunk = 32768  # tune if needed\n",
    "            for start in range(0, flat_trajectories.shape[0], chunk):\n",
    "                end = min(start + chunk, flat_trajectories.shape[0])\n",
    "                batch_tensor = torch.from_numpy(flat_trajectories[start:end]).float().to(device)\n",
    "                logits_list.append(model(batch_tensor).cpu())\n",
    "            all_logits = torch.cat(logits_list, dim=0)\n",
    "            all_error_probs = torch.sigmoid(all_logits).numpy().flatten()\n",
    "\n",
    "        error_probs_per_frame = all_error_probs.reshape(len(failed_indices), n_code)\n",
    "        for idx, i in enumerate(failed_indices):\n",
    "            v_col = v_messages[:, i]\n",
    "            base_decision = failed_decoded_words[idx]\n",
    "\n",
    "            errors_base_frame = np.count_nonzero(get_message(G, base_decision) != v_col)\n",
    "            total_errors_cnn_osd -= errors_base_frame\n",
    "\n",
    "            reliability_metric = 1.0 - error_probs_per_frame[idx]\n",
    "            d_cnn_osd = osd_rescue_order3(H, base_decision, reliability_metric)\n",
    "            total_errors_cnn_osd += np.count_nonzero(get_message(G, d_cnn_osd) != v_col)\n",
    "\n",
    "    ber_baseline.append(total_errors_baseline / (k_info * n_trials))\n",
    "    ber_std_osd.append(total_errors_std_osd / (k_info * n_trials))\n",
    "    ber_cnn_osd.append(total_errors_cnn_osd / (k_info * n_trials))\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"SNR={snr:.2f} dB finished in {elapsed:.1f}s.  NMS:{ber_baseline[-1]:.8e}  StdOSD:{ber_std_osd[-1]:.8e}  CNN-OSD:{ber_cnn_osd[-1]:.8e}\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.semilogy(snrs_db, ber_baseline, 'o:', label='Baseline NMS')\n",
    "plt.semilogy(snrs_db, ber_std_osd, 's--', label='NMS + Standard OSD')\n",
    "plt.semilogy(snrs_db, ber_cnn_osd, '^-', label='NMS + CNN-Enhanced OSD')\n",
    "plt.xlabel('SNR (dB)'); plt.ylabel('BER'); plt.grid(True, which='both', ls='--', alpha=0.5); plt.legend(); plt.ylim(1e-5, 1)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
