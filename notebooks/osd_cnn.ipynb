{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置工作目录到项目根目录，确保相对路径（`models/` 等）可用。\n",
    "- 固定随机种子，选择计算设备（GPU优先）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Ensure working directory is project root (adjust if needed)\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(\"..\")))\n",
    "os.chdir(project_root)\n",
    "print(f\"CWD: {os.getcwd()}\")\n",
    "\n",
    "# Global seed and device\n",
    "seed = np.random.RandomState(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 生成/加载校验矩阵 `H` 与生成矩阵 `G`，打印 `(n, k)` 与码率。\n",
    "- 设置全局参数：`USE_APP_LLR_FOR_OSD`、`max_iter_nms`、训练SNR、batch大小等。\n",
    "- 指定模型保存路径 `models/dia_cnn.pth`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pyldpc import make_ldpc, encode, decode, get_message\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# LDPC code params\n",
    "n = 128\n",
    "d_v = 4\n",
    "d_c = 8\n",
    "H, G = make_ldpc(n, d_v, d_c, seed=seed, systematic=True, sparse=True)\n",
    "n_code, k_info = G.shape\n",
    "print(f\"LDPC: n={n_code}, k={k_info}, R={k_info/n_code:.2f}\")\n",
    "\n",
    "# Global configs\n",
    "USE_APP_LLR_FOR_OSD = False\n",
    "max_iter_nms = 12\n",
    "snr_train_db = 2.7\n",
    "epochs = 20\n",
    "batch_size = 256\n",
    "\n",
    "# Model path\n",
    "model_dir = os.path.join(\"models\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, \"dia_cnn.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第2格说明：NMS解码与LLR轨迹记录\n",
    "- 构建Tanner图邻接，执行规范化最小和（NMS）迭代。\n",
    "- 每轮更新后记录变量结点后验LLR，形成 `(n, T)` 轨迹（第0列为信道LLR）。\n",
    "- 支持综合校验早停。`alpha` 可按论文调整。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: NMS Decoder with Trajectory\n",
    "import numpy as np\n",
    "\n",
    "def decode_with_trajectory(H, y, snr, max_iter):\n",
    "    \"\"\"\n",
    "    Normalized Min-Sum (NMS) with a posteriori LLR trajectory.\n",
    "    Returns final hard bits and llrs_trajectory of shape (n, max_iter+1).\n",
    "    \"\"\"\n",
    "    m_checks, n_vars = H.shape\n",
    "\n",
    "    # Build Tanner graph adjacency\n",
    "    if hasattr(H, 'nonzero'):\n",
    "        rows, cols = H.nonzero()\n",
    "    else:\n",
    "        rows, cols = np.nonzero(H)\n",
    "    check_to_vars = [[] for _ in range(m_checks)]\n",
    "    var_to_checks = [[] for _ in range(n_vars)]\n",
    "    for r, c in zip(rows, cols):\n",
    "        check_to_vars[r].append(c)\n",
    "        var_to_checks[c].append(r)\n",
    "\n",
    "    # Rate and Eb/N0\n",
    "    R = float(k_info) / float(n_code)\n",
    "    EbN0_linear = 10.0 ** (float(snr) / 10.0)\n",
    "\n",
    "    # Channel LLR (BPSK+AWGN approx): LLR = 4*R*EbN0*y\n",
    "    channel_llr = 4.0 * R * EbN0_linear * y.astype(float)\n",
    "\n",
    "    llrs_trajectory = np.zeros((n_vars, max_iter + 1), dtype=float)\n",
    "    llrs_trajectory[:, 0] = channel_llr\n",
    "\n",
    "    L_q = {}\n",
    "    R_msg = {}\n",
    "    for v in range(n_vars):\n",
    "        for c in var_to_checks[v]:\n",
    "            L_q[(c, v)] = channel_llr[v]\n",
    "            R_msg[(c, v)] = 0.0\n",
    "\n",
    "    alpha = 0.8  # normalization factor\n",
    "\n",
    "    a_posteriori = channel_llr.copy()\n",
    "    for it in range(1, max_iter + 1):\n",
    "        # Check node update\n",
    "        for c in range(m_checks):\n",
    "            vars_c = check_to_vars[c]\n",
    "            if not vars_c:\n",
    "                continue\n",
    "            abs_vals = [abs(L_q[(c, v)]) for v in vars_c]\n",
    "            signs = [1.0 if L_q[(c, v)] >= 0 else -1.0 for v in vars_c]\n",
    "            sign_prod_all = 1.0\n",
    "            for s in signs:\n",
    "                sign_prod_all *= s\n",
    "            if len(abs_vals) == 1:\n",
    "                min1, min2 = abs_vals[0], float('inf')\n",
    "                idx_min1 = 0\n",
    "            else:\n",
    "                idx_min1 = int(np.argmin(abs_vals))\n",
    "                min1 = abs_vals[idx_min1]\n",
    "                min2 = min(abs_vals[i] for i in range(len(abs_vals)) if i != idx_min1)\n",
    "            for i, v in enumerate(vars_c):\n",
    "                use_min = min2 if i == idx_min1 else min1\n",
    "                sign_extrinsic = sign_prod_all * (1.0 if L_q[(c, v)] >= 0 else -1.0) * (-1.0)\n",
    "                R_msg[(c, v)] = alpha * sign_extrinsic * use_min\n",
    "\n",
    "        # Variable node update\n",
    "        a_posteriori = channel_llr.copy()\n",
    "        for v in range(n_vars):\n",
    "            sum_R = 0.0\n",
    "            for c in var_to_checks[v]:\n",
    "                sum_R += R_msg[(c, v)]\n",
    "            a_posteriori[v] += sum_R\n",
    "            for c in var_to_checks[v]:\n",
    "                L_q[(c, v)] = a_posteriori[v] - R_msg[(c, v)]\n",
    "\n",
    "        llrs_trajectory[:, it] = a_posteriori\n",
    "        hard_bits = (a_posteriori < 0).astype(int)\n",
    "        if np.all((H.dot(hard_bits) % 2) == 0):\n",
    "            break\n",
    "\n",
    "    final_bits = (a_posteriori < 0).astype(int)\n",
    "    return final_bits, llrs_trajectory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第3格说明：训练数据生成（仅失败样本）\n",
    "- 发送全零码字，通过AWGN信道多次采样；\n",
    "- 对每次采样运行 NMS 并记录LLR轨迹；\n",
    "- 仅当NMS失败时，收集每个比特的轨迹作为训练样本，以提升训练有效性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Training Data Generation (Failures Only)\n",
    "\n",
    "def is_valid_codeword(H, c):\n",
    "    if c is None:\n",
    "        return False\n",
    "    return np.all((H.dot(c) % 2) == 0)\n",
    "\n",
    "\n",
    "def generate_training_data_from_failures(n_failures_target, snr_db, max_iter_nms):\n",
    "    print(f\"Generating training data from NMS failures at SNR={snr_db} dB...\")\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "\n",
    "    # All-zero message\n",
    "    v_message = np.zeros((k_info, 1))\n",
    "    true_codeword = encode(G, v_message, snr_db).flatten()\n",
    "\n",
    "    n_failures_found = 0\n",
    "    n_sims_run = 0\n",
    "    while n_failures_found < n_failures_target:\n",
    "        # Generate one noisy observation for the all-zero codeword\n",
    "        y_noisy = encode(G, v_message, snr_db, seed=seed).flatten()\n",
    "\n",
    "        decoded_word, llr_trajectory = decode_with_trajectory(H, y_noisy, snr_db, max_iter=max_iter_nms)\n",
    "        n_sims_run += 1\n",
    "        if not is_valid_codeword(H, decoded_word):\n",
    "            n_failures_found += 1\n",
    "            for i in range(n_code):\n",
    "                X_train_list.append(llr_trajectory[i, :])\n",
    "                y_train_list.append(true_codeword[i])\n",
    "        if n_sims_run % 5000 == 0:\n",
    "            print(f\"  Sims run: {n_sims_run}, Failures: {n_failures_found}/{n_failures_target}\")\n",
    "\n",
    "    X_train = np.array(X_train_list).reshape(-1, 1, max_iter_nms + 1)\n",
    "    y_train = np.array(y_train_list).reshape(-1, 1, 1)\n",
    "    print(f\"Generated {X_train.shape[0]} samples from {n_failures_found} frame failures.\\n\")\n",
    "    return X_train, y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第4格说明：DIA CNN 模型定义\n",
    "- 一维卷积堆叠提取轨迹时序特征，输出为单一logit（bit为1的概率的对数几率）。\n",
    "- 输入形状为 `(batch, 1, T)`，其中 `T = max_iter_nms + 1`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: DIA CNN Model\n",
    "class DIA_CNN_Model(nn.Module):\n",
    "    def __init__(self, trajectory_length):\n",
    "        super(DIA_CNN_Model, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=32, out_channels=1, kernel_size=3, padding='same')\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(1 * trajectory_length, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate\n",
    "max_iter_nms = max_iter_nms\n",
    "model = DIA_CNN_Model(trajectory_length=max_iter_nms + 1).to(device)\n",
    "print(\"Model ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第5格说明：模型持久化与条件训练\n",
    "- 如果存在 `models/dia_cnn.pth` 则直接加载并跳过训练。\n",
    "- 否则：仅从NMS失败帧采样生成训练数据并训练，训练后保存权重，随后切换到推理模式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Persistence and Conditional Training\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Found existing model at {model_path}. Loading...\")\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"Loaded.\\n\")\n",
    "else:\n",
    "    print(\"No saved model. Generating failures and training...\")\n",
    "    X_train_np, y_train_np = generate_training_data_from_failures(\n",
    "        n_failures_target=4000, snr_db=snr_train_db, max_iter_nms=max_iter_nms\n",
    "    )\n",
    "    X_train = torch.from_numpy(X_train_np).float()\n",
    "    y_train = torch.from_numpy(y_train_np).float()\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(20):\n",
    "        epoch_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.view(-1, 1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(inputs)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/20, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    model.eval()\n",
    "    print(f\"Saved model to {model_path}.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第6格说明：解码器封装（OSD与推理）\n",
    "- `osd_rescue_order3`：按可靠度对最不可靠比特进行阶数1/2/3翻转救援（受 `L` 和 `max_triplets` 控制）。\n",
    "- `standard_osd_decoder`：可配置使用信道LLR或NMS最后一轮后验LLR作为可靠度。\n",
    "- `cnn_enhanced_decoder`：利用CNN输出的比特错误概率构造可靠度并执行OSD救援。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Decoders (OSD and Wrappers)\n",
    "from itertools import combinations\n",
    "\n",
    "def osd_rescue_order3(H, base_decision, reliability_vector, L=12, max_triplets=100):\n",
    "    idx_sorted = np.argsort(reliability_vector)\n",
    "    least_reliable = idx_sorted[:min(L, len(idx_sorted))]\n",
    "    if is_valid_codeword(H, base_decision):\n",
    "        return base_decision\n",
    "    # Order-1\n",
    "    for i in least_reliable:\n",
    "        cand = base_decision.copy(); cand[i] ^= 1\n",
    "        if is_valid_codeword(H, cand):\n",
    "            return cand\n",
    "    # Order-2\n",
    "    for i, j in combinations(least_reliable, 2):\n",
    "        cand = base_decision.copy(); cand[i] ^= 1; cand[j] ^= 1\n",
    "        if is_valid_codeword(H, cand):\n",
    "            return cand\n",
    "    # Order-3 (limited)\n",
    "    triplets_checked = 0\n",
    "    for i, j, k in combinations(least_reliable, 3):\n",
    "        cand = base_decision.copy(); cand[i] ^= 1; cand[j] ^= 1; cand[k] ^= 1\n",
    "        if is_valid_codeword(H, cand):\n",
    "            return cand\n",
    "        triplets_checked += 1\n",
    "        if triplets_checked >= max_triplets:\n",
    "            break\n",
    "    return base_decision\n",
    "\n",
    "\n",
    "def baseline_nms_decoder(H, y, snr):\n",
    "    return decode(H, y, snr, maxiter=max_iter_nms)\n",
    "\n",
    "\n",
    "def standard_osd_decoder(H, y, snr, predecoded=None, llr_traj=None):\n",
    "    decoded = predecoded if predecoded is not None else baseline_nms_decoder(H, y, snr)\n",
    "    if is_valid_codeword(H, decoded):\n",
    "        return decoded\n",
    "    reliability_vec = np.abs(llr_traj[:, -1]) if (USE_APP_LLR_FOR_OSD and llr_traj is not None) else np.abs(y)\n",
    "    return osd_rescue_order3(H, decoded, reliability_vec)\n",
    "\n",
    "\n",
    "def cnn_enhanced_decoder(H, y, snr, model, dev, predecoded=None, llr_traj=None):\n",
    "    if predecoded is None or llr_traj is None:\n",
    "        decoded, llr_trajectory = decode_with_trajectory(H, y, snr, max_iter=max_iter_nms)\n",
    "    else:\n",
    "        decoded, llr_trajectory = predecoded, llr_traj\n",
    "    if is_valid_codeword(H, decoded):\n",
    "        return decoded\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        traj_tensor = torch.from_numpy(llr_trajectory.T).float().reshape(n_code, 1, -1).to(dev)\n",
    "        logits = model(traj_tensor)\n",
    "        error_probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "    reliability_metric = 1.0 - error_probs\n",
    "    return osd_rescue_order3(H, decoded, reliability_metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第7格说明：BER仿真与绘图（批量优化）\n",
    "- 批量生成 `n_trials` 帧的接收向量，逐帧运行一次 NMS，收集失败帧。\n",
    "- 标准OSD：仅对失败帧进行救援（可靠度可选信道LLR/后验LLR）。\n",
    "- CNN-OSD：对失败帧的比特轨迹做一次（分块）前向，得到错误概率并执行OSD救援。\n",
    "- 统计三条曲线的BER并绘图。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: BER Simulation (Batched)\n",
    "snrs_db = np.arange(2.0, 4.5, 0.5)\n",
    "n_trials = 20000\n",
    "ber_baseline, ber_std_osd, ber_cnn_osd = [], [], []\n",
    "\n",
    "v_messages = seed.randint(2, size=(k_info, n_trials))\n",
    "\n",
    "for snr in snrs_db:\n",
    "    start_time = time.time()\n",
    "    total_errors_baseline, total_errors_std_osd, total_errors_cnn_osd = 0, 0, 0\n",
    "\n",
    "    y_noisy_batch = encode(G, v_messages, snr, seed=seed)\n",
    "\n",
    "    failed_indices, failed_trajectories, failed_decoded_words = [], [], []\n",
    "\n",
    "    # Stage 1: NMS and Standard OSD\n",
    "    for i in range(n_trials):\n",
    "        y_col, v_col = y_noisy_batch[:, i], v_messages[:, i]\n",
    "        d_nms, llr_traj = decode_with_trajectory(H, y_col, snr, max_iter=max_iter_nms)\n",
    "\n",
    "        errors_this = np.count_nonzero(get_message(G, d_nms) != v_col)\n",
    "        total_errors_baseline += errors_this\n",
    "\n",
    "        if not is_valid_codeword(H, d_nms):\n",
    "            failed_indices.append(i)\n",
    "            failed_trajectories.append(llr_traj)\n",
    "            failed_decoded_words.append(d_nms)\n",
    "\n",
    "            reliability_vec = np.abs(llr_traj[:, -1]) if USE_APP_LLR_FOR_OSD else np.abs(y_col)\n",
    "            d_std_osd = osd_rescue_order3(H, d_nms, reliability_vec)\n",
    "            total_errors_std_osd += np.count_nonzero(get_message(G, d_std_osd) != v_col)\n",
    "        else:\n",
    "            total_errors_std_osd += errors_this\n",
    "\n",
    "    # Start CNN-OSD from baseline errors\n",
    "    total_errors_cnn_osd = total_errors_baseline\n",
    "\n",
    "    # Stage 2: CNN rescue on failed frames\n",
    "    if len(failed_indices) > 0:\n",
    "        print(f\"SNR={snr:.2f} dB, NMS failures: {len(failed_indices)}/{n_trials}\")\n",
    "        batch_trajectories = np.array(failed_trajectories)\n",
    "        flat_trajectories = batch_trajectories.reshape(-1, 1, max_iter_nms + 1)\n",
    "\n",
    "        # Optional: chunking to avoid OOM\n",
    "        with torch.no_grad():\n",
    "            logits_list = []\n",
    "            chunk = 32768  # tune if needed\n",
    "            for start in range(0, flat_trajectories.shape[0], chunk):\n",
    "                end = min(start + chunk, flat_trajectories.shape[0])\n",
    "                batch_tensor = torch.from_numpy(flat_trajectories[start:end]).float().to(device)\n",
    "                logits_list.append(model(batch_tensor).cpu())\n",
    "            all_logits = torch.cat(logits_list, dim=0)\n",
    "            all_error_probs = torch.sigmoid(all_logits).numpy().flatten()\n",
    "\n",
    "        error_probs_per_frame = all_error_probs.reshape(len(failed_indices), n_code)\n",
    "        for idx, i in enumerate(failed_indices):\n",
    "            v_col = v_messages[:, i]\n",
    "            base_decision = failed_decoded_words[idx]\n",
    "\n",
    "            errors_base_frame = np.count_nonzero(get_message(G, base_decision) != v_col)\n",
    "            total_errors_cnn_osd -= errors_base_frame\n",
    "\n",
    "            reliability_metric = 1.0 - error_probs_per_frame[idx]\n",
    "            d_cnn_osd = osd_rescue_order3(H, base_decision, reliability_metric)\n",
    "            total_errors_cnn_osd += np.count_nonzero(get_message(G, d_cnn_osd) != v_col)\n",
    "\n",
    "    ber_baseline.append(total_errors_baseline / (k_info * n_trials))\n",
    "    ber_std_osd.append(total_errors_std_osd / (k_info * n_trials))\n",
    "    ber_cnn_osd.append(total_errors_cnn_osd / (k_info * n_trials))\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"SNR={snr:.2f} dB finished in {elapsed:.1f}s.  NMS:{ber_baseline[-1]:.8e}  StdOSD:{ber_std_osd[-1]:.8e}  CNN-OSD:{ber_cnn_osd[-1]:.8e}\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.semilogy(snrs_db, ber_baseline, 'o:', label='Baseline NMS')\n",
    "plt.semilogy(snrs_db, ber_std_osd, 's--', label='NMS + Standard OSD')\n",
    "plt.semilogy(snrs_db, ber_cnn_osd, '^-', label='NMS + CNN-Enhanced OSD')\n",
    "plt.xlabel('SNR (dB)'); plt.ylabel('BER'); plt.grid(True, which='both', ls='--', alpha=0.5); plt.legend(); plt.ylim(1e-5, 1)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
