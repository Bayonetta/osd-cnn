{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置工作目录到项目根目录，确保相对路径（`models/` 等）可用。\n",
    "- 固定随机种子，选择计算设备（GPU优先）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: z:\\WorkSpace\\osd-cnn\\notebooks\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Ensure working directory is project root (adjust if needed)\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(\"..\")))\n",
    "os.chdir(project_root)\n",
    "print(f\"CWD: {os.getcwd()}\")\n",
    "\n",
    "# Global seed and device\n",
    "seed = np.random.RandomState(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 生成/加载校验矩阵 `H` 与生成矩阵 `G`，打印 `(n, k)` 与码率。\n",
    "- 设置全局参数：`USE_APP_LLR_FOR_OSD`、`max_iter_nms`、训练SNR、batch大小等。\n",
    "- 指定模型保存路径 `models/dia_cnn.pth`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDPC: n=128, k=67, R=0.52\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pyldpc import make_ldpc, encode, decode, get_message\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# LDPC code params\n",
    "n = 128\n",
    "d_v = 4\n",
    "d_c = 8\n",
    "H, G = make_ldpc(n, d_v, d_c, seed=seed, systematic=True, sparse=True)\n",
    "n_code, k_info = G.shape\n",
    "print(f\"LDPC: n={n_code}, k={k_info}, R={k_info/n_code:.2f}\")\n",
    "\n",
    "# Global configs\n",
    "USE_APP_LLR_FOR_OSD = False\n",
    "max_iter_nms = 12\n",
    "snr_train_db = 2.7\n",
    "epochs = 20\n",
    "batch_size = 256\n",
    "\n",
    "# Model path\n",
    "model_dir = os.path.join(\"models\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, \"dia_cnn.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构建Tanner图邻接，执行规范化最小和（NMS）迭代。\n",
    "- 支持综合校验早停。`alpha` 可按论文调整。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def decode_with_trajectory(H, y, snr, max_iter):\n",
    "    \"\"\"\n",
    "    规范化最小和（NMS）解码，返回：\n",
    "      - final_codeword: 最终硬判决码字（0/1）\n",
    "      - llrs_trajectory: 形状 (n, max_iter + 1) 的后验LLR轨迹\n",
    "    \"\"\"\n",
    "    m_checks, n_vars = H.shape\n",
    "\n",
    "    # --- Tanner图邻接表构建 (这部分是正确的) ---\n",
    "    if hasattr(H, 'nonzero'):\n",
    "        rows, cols = H.nonzero()\n",
    "    else: # For dense numpy array\n",
    "        rows, cols = np.nonzero(H)\n",
    "    check_to_vars = [[] for _ in range(m_checks)]\n",
    "    var_to_checks = [[] for _ in range(n_vars)]\n",
    "    for r, c in zip(rows, cols):\n",
    "        check_to_vars[r].append(c)\n",
    "        var_to_checks[c].append(r)\n",
    "\n",
    "    # --- 初始LLR计算 (这部分是正确的) ---\n",
    "    R = float(k_info) / float(n_code)\n",
    "    EbN0_linear = 10.0 ** (float(snr) / 10.0)\n",
    "    # 假设 y 是 BPSK (+1/-1) + 噪声, 转换为 LLR\n",
    "    # pyldpc.encode返回的就是这种y\n",
    "    channel_llr = 4.0 * R * EbN0_linear * y.astype(float)\n",
    "    \n",
    "    llrs_trajectory = np.zeros((n_vars, max_iter + 1), dtype=float)\n",
    "    llrs_trajectory[:, 0] = channel_llr\n",
    "\n",
    "    # --- 消息初始化 (这部分是正确的) ---\n",
    "    v2c_msgs = np.zeros_like(H.toarray(), dtype=float)\n",
    "    for r, c in zip(rows, cols):\n",
    "        v2c_msgs[r, c] = channel_llr[c]\n",
    "    c2v_msgs = np.zeros_like(v2c_msgs)\n",
    "    \n",
    "    alpha = 0.8  # 规范化因子\n",
    "\n",
    "    # --- 迭代更新 ---\n",
    "    a_posteriori = channel_llr.copy()\n",
    "    for it in range(1, max_iter + 1):\n",
    "        # --- 校验节点更新 (Check Node Update) ---\n",
    "        for c in range(m_checks):\n",
    "            connected_vars = check_to_vars[c]\n",
    "            if not connected_vars: continue\n",
    "\n",
    "            # 获取所有连接到此校验节点的消息\n",
    "            incoming_msgs = [v2c_msgs[c, v] for v in connected_vars]\n",
    "            \n",
    "            # --- 关键Bug修正 ---\n",
    "            for i, v_target in enumerate(connected_vars):\n",
    "                # 提取除当前目标外的所有其它消息\n",
    "                other_msgs = incoming_msgs[:i] + incoming_msgs[i+1:]\n",
    "                \n",
    "                # 计算符号乘积\n",
    "                sign_prod = np.prod(np.sign(other_msgs))\n",
    "                \n",
    "                # 找到最小绝对值\n",
    "                min_abs = min(np.abs(other_msgs)) if other_msgs else 0.0\n",
    "                \n",
    "                # 更新校验到变量的消息\n",
    "                c2v_msgs[c, v_target] = alpha * sign_prod * min_abs\n",
    "\n",
    "        # --- 变量节点更新 (Variable Node Update) ---\n",
    "        for v in range(n_vars):\n",
    "            connected_checks = var_to_checks[v]\n",
    "            if not connected_checks: continue\n",
    "\n",
    "            # 累加来自所有校验节点的消息\n",
    "            sum_c2v = sum(c2v_msgs[c, v] for c in connected_checks)\n",
    "            \n",
    "            # 更新后验LLR\n",
    "            a_posteriori[v] = channel_llr[v] + sum_c2v\n",
    "            \n",
    "            # 更新变量到校验节点的消息（外信息）\n",
    "            for c in connected_checks:\n",
    "                v2c_msgs[c, v] = a_posteriori[v] - c2v_msgs[c, v]\n",
    "\n",
    "        llrs_trajectory[:, it] = a_posteriori\n",
    "\n",
    "        hard_bits = (a_posteriori < 0).astype(int)\n",
    "        if np.all((H.dot(hard_bits) % 2) == 0):\n",
    "            break\n",
    "\n",
    "    final_bits = (a_posteriori < 0).astype(int)\n",
    "    return final_bits, llrs_trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 发送全零码字，通过AWGN信道多次采样；\n",
    "- 对每次采样运行 NMS 并记录LLR轨迹；\n",
    "- 仅当NMS失败时，收集每个比特的轨迹作为训练样本，以提升训练有效性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_codeword(H, c):\n",
    "    if c is None:\n",
    "        return False\n",
    "    return np.all((H.dot(c) % 2) == 0)\n",
    "\n",
    "\n",
    "def generate_training_data_from_failures(n_failures_target, snr_db, max_iter_nms):\n",
    "    print(f\"Generating training data from NMS failures at SNR={snr_db} dB...\")\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "\n",
    "    # All-zero message\n",
    "    v_message = np.zeros((k_info, 1))\n",
    "    true_codeword = encode(G, v_message, snr_db).flatten()\n",
    "\n",
    "    n_failures_found = 0\n",
    "    n_sims_run = 0\n",
    "    while n_failures_found < n_failures_target:\n",
    "        # Generate one noisy observation for the all-zero codeword\n",
    "        y_noisy = encode(G, v_message, snr_db, seed=seed).flatten()\n",
    "\n",
    "        decoded_word, llr_trajectory = decode_with_trajectory(H, y_noisy, snr_db, max_iter=max_iter_nms)\n",
    "        n_sims_run += 1\n",
    "        if not is_valid_codeword(H, decoded_word):\n",
    "            n_failures_found += 1\n",
    "            for i in range(n_code):\n",
    "                X_train_list.append(llr_trajectory[i, :])\n",
    "                y_train_list.append(true_codeword[i])\n",
    "        if n_sims_run % 5000 == 0:\n",
    "            print(f\"  Sims run: {n_sims_run}, Failures: {n_failures_found}/{n_failures_target}\")\n",
    "\n",
    "    X_train = np.array(X_train_list).reshape(-1, 1, max_iter_nms + 1)\n",
    "    y_train = np.array(y_train_list).reshape(-1, 1, 1)\n",
    "    print(f\"Generated {X_train.shape[0]} samples from {n_failures_found} frame failures.\\n\")\n",
    "    return X_train, y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 一维卷积堆叠提取轨迹时序特征，输出为单一logit（bit为1的概率的对数几率）。\n",
    "- 输入形状为 `(batch, 1, T)`，其中 `T = max_iter_nms + 1`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready\n"
     ]
    }
   ],
   "source": [
    "class DIA_CNN_Model(nn.Module):\n",
    "    def __init__(self, trajectory_length):\n",
    "        super(DIA_CNN_Model, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=32, out_channels=1, kernel_size=3, padding='same')\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(1 * trajectory_length, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate\n",
    "max_iter_nms = max_iter_nms\n",
    "model = DIA_CNN_Model(trajectory_length=max_iter_nms + 1).to(device)\n",
    "print(\"Model ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果存在 `models/dia_cnn.pth` 则直接加载并跳过训练。\n",
    "- 否则：仅从NMS失败帧采样生成训练数据并训练，训练后保存权重，随后切换到推理模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved model. Generating failures and training...\n",
      "Generating training data from NMS failures at SNR=2.7 dB...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo saved model. Generating failures and training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     X_train_np, y_train_np \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_training_data_from_failures\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_failures_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnr_db\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msnr_train_db\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter_nms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter_nms\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(X_train_np)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     14\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(y_train_np)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m, in \u001b[0;36mgenerate_training_data_from_failures\u001b[1;34m(n_failures_target, snr_db, max_iter_nms)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n_failures_found \u001b[38;5;241m<\u001b[39m n_failures_target:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Generate one noisy observation for the all-zero codeword\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     y_noisy \u001b[38;5;241m=\u001b[39m encode(G, v_message, snr_db, seed\u001b[38;5;241m=\u001b[39mseed)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m---> 22\u001b[0m     decoded_word, llr_trajectory \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_with_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnr_db\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter_nms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     n_sims_run \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_valid_codeword(H, decoded_word):\n",
      "Cell \u001b[1;32mIn[3], line 33\u001b[0m, in \u001b[0;36mdecode_with_trajectory\u001b[1;34m(H, y, snr, max_iter)\u001b[0m\n\u001b[0;32m     30\u001b[0m llrs_trajectory[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m channel_llr\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# --- 消息初始化 (这部分是正确的) ---\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m v2c_msgs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[43mH\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m(), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(rows, cols):\n\u001b[0;32m     35\u001b[0m     v2c_msgs[r, c] \u001b[38;5;241m=\u001b[39m channel_llr[c]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": [
    "if os.path.exists(model_path):\n",
    "    print(f\"Found existing model at {model_path}. Loading...\")\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"Loaded.\\n\")\n",
    "else:\n",
    "    print(\"No saved model. Generating failures and training...\")\n",
    "    X_train_np, y_train_np = generate_training_data_from_failures(\n",
    "        n_failures_target=4000, snr_db=snr_train_db, max_iter_nms=max_iter_nms\n",
    "    )\n",
    "    X_train = torch.from_numpy(X_train_np).float()\n",
    "    y_train = torch.from_numpy(y_train_np).float()\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(20):\n",
    "        epoch_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.view(-1, 1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(inputs)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/20, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    model.eval()\n",
    "    print(f\"Saved model to {model_path}.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `osd_rescue_order3`：按可靠度对最不可靠比特进行阶数1/2/3翻转救援（受 `L` 和 `max_triplets` 控制）。\n",
    "- `standard_osd_decoder`：可配置使用信道LLR或NMS最后一轮后验LLR作为可靠度。\n",
    "- `cnn_enhanced_decoder`：利用CNN输出的比特错误概率构造可靠度并执行OSD救援。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def osd_rescue_order3(H, base_decision, reliability_vector, L=12, max_triplets=100):\n",
    "    idx_sorted = np.argsort(reliability_vector)\n",
    "    least_reliable = idx_sorted[:min(L, len(idx_sorted))]\n",
    "    if is_valid_codeword(H, base_decision):\n",
    "        return base_decision\n",
    "    # Order-1\n",
    "    for i in least_reliable:\n",
    "        cand = base_decision.copy(); cand[i] ^= 1\n",
    "        if is_valid_codeword(H, cand):\n",
    "            return cand\n",
    "    # Order-2\n",
    "    for i, j in combinations(least_reliable, 2):\n",
    "        cand = base_decision.copy(); cand[i] ^= 1; cand[j] ^= 1\n",
    "        if is_valid_codeword(H, cand):\n",
    "            return cand\n",
    "    # Order-3 (limited)\n",
    "    triplets_checked = 0\n",
    "    for i, j, k in combinations(least_reliable, 3):\n",
    "        cand = base_decision.copy(); cand[i] ^= 1; cand[j] ^= 1; cand[k] ^= 1\n",
    "        if is_valid_codeword(H, cand):\n",
    "            return cand\n",
    "        triplets_checked += 1\n",
    "        if triplets_checked >= max_triplets:\n",
    "            break\n",
    "    return base_decision\n",
    "\n",
    "\n",
    "def baseline_nms_decoder(H, y, snr):\n",
    "    return decode(H, y, snr, maxiter=max_iter_nms)\n",
    "\n",
    "\n",
    "def standard_osd_decoder(H, y, snr, predecoded=None, llr_traj=None):\n",
    "    decoded = predecoded if predecoded is not None else baseline_nms_decoder(H, y, snr)\n",
    "    if is_valid_codeword(H, decoded):\n",
    "        return decoded\n",
    "    reliability_vec = np.abs(llr_traj[:, -1]) if (USE_APP_LLR_FOR_OSD and llr_traj is not None) else np.abs(y)\n",
    "    return osd_rescue_order3(H, decoded, reliability_vec)\n",
    "\n",
    "\n",
    "def cnn_enhanced_decoder(H, y, snr, model, dev, predecoded=None, llr_traj=None):\n",
    "    if predecoded is None or llr_traj is None:\n",
    "        decoded, llr_trajectory = decode_with_trajectory(H, y, snr, max_iter=max_iter_nms)\n",
    "    else:\n",
    "        decoded, llr_trajectory = predecoded, llr_traj\n",
    "    if is_valid_codeword(H, decoded):\n",
    "        return decoded\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        traj_tensor = torch.from_numpy(llr_trajectory.T).float().reshape(n_code, 1, -1).to(dev)\n",
    "        logits = model(traj_tensor)\n",
    "        error_probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "    reliability_metric = 1.0 - error_probs\n",
    "    return osd_rescue_order3(H, decoded, reliability_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 批量生成 `n_trials` 帧的接收向量，逐帧运行一次 NMS，收集失败帧。\n",
    "- 标准OSD：仅对失败帧进行救援（可靠度可选信道LLR/后验LLR）。\n",
    "- CNN-OSD：对失败帧的比特轨迹做一次（分块）前向，得到错误概率并执行OSD救援。\n",
    "- 统计三条曲线的BER并绘图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "snrs_db = np.arange(2.0, 4.5, 0.5)\n",
    "n_trials = 20000\n",
    "ber_baseline, ber_std_osd, ber_cnn_osd = [], [], []\n",
    "\n",
    "v_messages = seed.randint(2, size=(k_info, n_trials))\n",
    "\n",
    "for snr in snrs_db:\n",
    "    start_time = time.time()\n",
    "    total_errors_baseline, total_errors_std_osd, total_errors_cnn_osd = 0, 0, 0\n",
    "\n",
    "    y_noisy_batch = encode(G, v_messages, snr, seed=seed)\n",
    "\n",
    "    failed_indices, failed_trajectories, failed_decoded_words = [], [], []\n",
    "\n",
    "    # Stage 1: NMS and Standard OSD\n",
    "    for i in range(n_trials):\n",
    "        y_col, v_col = y_noisy_batch[:, i], v_messages[:, i]\n",
    "        d_nms, llr_traj = decode_with_trajectory(H, y_col, snr, max_iter=max_iter_nms)\n",
    "\n",
    "        errors_this = np.count_nonzero(get_message(G, d_nms) != v_col)\n",
    "        total_errors_baseline += errors_this\n",
    "\n",
    "        if not is_valid_codeword(H, d_nms):\n",
    "            failed_indices.append(i)\n",
    "            failed_trajectories.append(llr_traj)\n",
    "            failed_decoded_words.append(d_nms)\n",
    "\n",
    "            reliability_vec = np.abs(llr_traj[:, -1]) if USE_APP_LLR_FOR_OSD else np.abs(y_col)\n",
    "            d_std_osd = osd_rescue_order3(H, d_nms, reliability_vec)\n",
    "            total_errors_std_osd += np.count_nonzero(get_message(G, d_std_osd) != v_col)\n",
    "        else:\n",
    "            total_errors_std_osd += errors_this\n",
    "\n",
    "    # Start CNN-OSD from baseline errors\n",
    "    total_errors_cnn_osd = total_errors_baseline\n",
    "\n",
    "    # Stage 2: CNN rescue on failed frames\n",
    "    if len(failed_indices) > 0:\n",
    "        print(f\"SNR={snr:.2f} dB, NMS failures: {len(failed_indices)}/{n_trials}\")\n",
    "        batch_trajectories = np.array(failed_trajectories)\n",
    "        flat_trajectories = batch_trajectories.reshape(-1, 1, max_iter_nms + 1)\n",
    "\n",
    "        # Optional: chunking to avoid OOM\n",
    "        with torch.no_grad():\n",
    "            logits_list = []\n",
    "            chunk = 32768  # tune if needed\n",
    "            for start in range(0, flat_trajectories.shape[0], chunk):\n",
    "                end = min(start + chunk, flat_trajectories.shape[0])\n",
    "                batch_tensor = torch.from_numpy(flat_trajectories[start:end]).float().to(device)\n",
    "                logits_list.append(model(batch_tensor).cpu())\n",
    "            all_logits = torch.cat(logits_list, dim=0)\n",
    "            all_error_probs = torch.sigmoid(all_logits).numpy().flatten()\n",
    "\n",
    "        error_probs_per_frame = all_error_probs.reshape(len(failed_indices), n_code)\n",
    "        for idx, i in enumerate(failed_indices):\n",
    "            v_col = v_messages[:, i]\n",
    "            base_decision = failed_decoded_words[idx]\n",
    "\n",
    "            errors_base_frame = np.count_nonzero(get_message(G, base_decision) != v_col)\n",
    "            total_errors_cnn_osd -= errors_base_frame\n",
    "\n",
    "            reliability_metric = 1.0 - error_probs_per_frame[idx]\n",
    "            d_cnn_osd = osd_rescue_order3(H, base_decision, reliability_metric)\n",
    "            total_errors_cnn_osd += np.count_nonzero(get_message(G, d_cnn_osd) != v_col)\n",
    "\n",
    "    ber_baseline.append(total_errors_baseline / (k_info * n_trials))\n",
    "    ber_std_osd.append(total_errors_std_osd / (k_info * n_trials))\n",
    "    ber_cnn_osd.append(total_errors_cnn_osd / (k_info * n_trials))\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"SNR={snr:.2f} dB finished in {elapsed:.1f}s.  NMS:{ber_baseline[-1]:.8e}  StdOSD:{ber_std_osd[-1]:.8e}  CNN-OSD:{ber_cnn_osd[-1]:.8e}\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.semilogy(snrs_db, ber_baseline, 'o:', label='Baseline NMS')\n",
    "plt.semilogy(snrs_db, ber_std_osd, 's--', label='NMS + Standard OSD')\n",
    "plt.semilogy(snrs_db, ber_cnn_osd, '^-', label='NMS + CNN-Enhanced OSD')\n",
    "plt.xlabel('SNR (dB)'); plt.ylabel('BER'); plt.grid(True, which='both', ls='--', alpha=0.5); plt.legend(); plt.ylim(1e-5, 1)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
